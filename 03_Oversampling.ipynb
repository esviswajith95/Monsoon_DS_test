{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling\n",
    "\n",
    "This notebook has the code for oversampling the data with SMOTE. Nothing from this notebook made it to the final modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedShuffleSplit, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"./../Training/X_train.csv\")\n",
    "labels = pd.read_csv(\"./../Training/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training feature matrix is (33050, 44)\n",
      "The shape of training labels is (33050, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of training feature matrix is {features.shape}\")\n",
    "print(f\"The shape of training labels is {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check to see if the rows in X and y corresponds to the same entities\n",
    "assert (features[\"Unique_ID\"] == labels[\"Unique_ID\"]).all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split\n",
    "\n",
    "We need to set aside a portion of our training data to perform evaluations for selecting the best model. It is best to do it in the outset itself to avoid any data leakage or bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels[\"Dependent_Variable\"]\n",
    "X = features.drop(\"Unique_ID\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42, stratify=y) # Random state is set to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is (26440, 43)\n",
      "The shape of y_train is (26440,)\n",
      "\n",
      "The shape of X_test is (6610, 43)\n",
      "The shape of y_test is (6610,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of X_train is {X_train.shape}\")\n",
    "print(f\"The shape of y_train is {y_train.shape}\")\n",
    "print()\n",
    "print(f\"The shape of X_test is {X_test.shape}\")\n",
    "print(f\"The shape of y_test is {y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, cv=5):\n",
    "    '''Trains the  model\n",
    "    Args:\n",
    "        model: Instance of the model to train\n",
    "        X_train: Training data feature matrix\n",
    "        y_train: Training data label vector\n",
    "        cv: Cross validation Scheme\n",
    "        \n",
    "    Returns:\n",
    "        trained model\n",
    "        cv_results\n",
    "    '''\n",
    "\n",
    "    \n",
    "    cv_results = cross_validate(model, X_train, y_train,\n",
    "                                cv=cv,\n",
    "                                scoring='roc_auc', \n",
    "                                return_train_score=True)\n",
    "\n",
    "    mean_train_score = cv_results[\"train_score\"].mean()\n",
    "    mean_val_score = cv_results[\"test_score\"].mean()\n",
    "    \n",
    "    std_train_score = cv_results[\"train_score\"].std()\n",
    "    std_val_score = cv_results[\"test_score\"].std()\n",
    "\n",
    "    print(f\"Cross validated training results for the model\")\n",
    "    print(f\"Train score: {mean_train_score} +/- {std_train_score}\" )\n",
    "    print(f\"Validation score: {mean_val_score} +/- {std_val_score}\" )\n",
    "\n",
    "    trained_model = model.fit(X_train, y_train)\n",
    "\n",
    "    return trained_model, cv_results\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "\n",
    "    '''Evaluates the  model\n",
    "    Args:\n",
    "        fitted_model: Instance of the model to train\n",
    "        X_test: Test data feature matrix\n",
    "        y_test: Test data label vector\n",
    "        \n",
    "    Returns:\n",
    "        metrics: A dictionary containing auc_roc score, accuracy, fpr and tpr\n",
    "    '''\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_scores = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_test, y_scores)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "    print(\"AUC-ROC score on test set: \", auc)\n",
    "    print(\"Accuracy score on test set: \", acc)\n",
    "    metrics = {\"auc_roc\": auc, \"accuracy\": acc, \"fpr\": fpr, \"tpr\": tpr}\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def tune_model(model, param_grid, cv=5):\n",
    "    '''Do hyper parameter tuning using GridSearch strategy\n",
    "    \n",
    "        Args:\n",
    "            model: Model to be tuned\n",
    "            param_grid: dict of parameters\n",
    "            X_train: Feature matrix\n",
    "            y_train: Label matrix\n",
    "        \n",
    "        Returns: \n",
    "            best parameters\n",
    "            best estimator\n",
    "        '''\n",
    "\n",
    "    search = GridSearchCV(model, param_grid = param_grid,\n",
    "                        cv = cv,\n",
    "                        scoring='roc_auc',\n",
    "                        return_train_score=True)\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = search.best_estimator_\n",
    "    cv_results = search.cv_results_\n",
    "    \n",
    "    print(\"Best parameters: \", search.best_params_)\n",
    "    \n",
    "    print(\"-------------------Best model performance --------------------------\")\n",
    "    \n",
    "    mean_train_score = search.cv_results_['mean_train_score'][search.best_index_]\n",
    "    mean_val_score = search.cv_results_['mean_test_score'][search.best_index_]\n",
    "    std_train_score = search.cv_results_['std_train_score'][search.best_index_]\n",
    "    std_val_score = search.cv_results_['std_test_score'][search.best_index_]\n",
    "\n",
    "    print(f\"Score of the model on the train set:\\n\"\n",
    "         f\"{mean_train_score:.3f} +/- {std_train_score:.6f}\")\n",
    "\n",
    "    print(f\"Score of the model on the validation set:\\n\"\n",
    "        f\"{mean_val_score:.3f} +/- {std_val_score:.6f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    return best_model, cv_results\n",
    "\n",
    "def compare_models(model_list, preprocess_pipe, cv):\n",
    "    '''Compare a list of models defined in model_list and return the auc, fpr and tpr\n",
    "        Args:\n",
    "            model_list: List of models to compare. Must be a list of dictionaries with keys;\n",
    "                            name - name of the model\n",
    "                            model - an instance of the estimator,\n",
    "                            param_grid - hyper parameter grid for hp tuning with gridsearch\n",
    "\n",
    "            preprocess_pipe: Pipeline object for preprocessing\n",
    "            cv: cross validation scheme\n",
    "\n",
    "        Returns:\n",
    "            logged_metrics: A dictionary with model names as keys\n",
    "    '''\n",
    "    logged_metrics = {}\n",
    "    for model in model_list:\n",
    "        model_name = model[\"name\"]\n",
    "        print(f\"Training {model_name} model...\")\n",
    "        model_pipe = Pipeline([('preprocess', preprocess_pipe),\n",
    "                                ('model', model[\"model\"])])\n",
    "        \n",
    "        print(f\"Tuning hyper parameters...\")\n",
    "        \n",
    "        param_grid = model[\"param_grid\"]\n",
    "        tuned_model, _ = tune_model(model_pipe, param_grid, cv=cv)\n",
    "        metrics = evaluate_model(tuned_model, X_test, y_test)\n",
    "\n",
    "        #store metrics for later use\n",
    "        logged_metrics[model_name] = metrics\n",
    "\n",
    "    return logged_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "cat_features = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8']\n",
    "\n",
    "# Numerical features -> N5 - N8 & N20, N27 removed because of high correlation. \n",
    "#                       N25 - N32 removed due to large number of missing values\n",
    "\n",
    "num_features = ['N1', 'N2', 'N3', 'N4', 'N9', 'N10', 'N10.1', 'N11', \n",
    "                'N12', 'N14','N15','N16', 'N17', 'N18', 'N19', 'N20',\n",
    "                 'N21', 'N22', 'N23', 'N24', 'N33', 'N34','N35']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is left the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline for categorical features\n",
    "\n",
    "# select the categorical features from the input\n",
    "select_cat_features = ColumnTransformer([('select_cat', 'passthrough', cat_features)])\n",
    "\n",
    "#OneHot encoding\n",
    "cat_transformers = Pipeline([('selector', select_cat_features),\n",
    "                            ('onehot', OneHotEncoder(handle_unknown='infrequent_if_exist')),\n",
    "                            ])\n",
    "\n",
    "## Pipeline for numerical featurees\n",
    "\n",
    "# select the numerical variables                            \n",
    "select_num_features = ColumnTransformer([('select_num', 'passthrough', num_features)])\n",
    "\n",
    "# Imputing for missing values, Scaling\n",
    "num_transformers = Pipeline([('selector', select_num_features),\n",
    "                            ('imputer', SimpleImputer()),\n",
    "                            ('scaler', StandardScaler()),\n",
    "                            ])\n",
    "\n",
    "## combining both pipelines\n",
    "preprocess_pipe = FeatureUnion([('cat', cat_transformers),\n",
    "                                ('num', num_transformers),\n",
    "                                ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validated training results for the model\n",
      "Train score: 1.0 +/- 4.965068306494546e-17\n",
      "Validation score: 0.7607794270530097 +/- 0.0035139542074323184\n",
      "AUC-ROC score on test set:  0.7617593974432431\n",
      "Accuracy score on test set:  0.7428139183055976\n"
     ]
    }
   ],
   "source": [
    "rf_model = Pipeline([(\"preprocessing\", preprocess_pipe),\n",
    "                    (\"model\", RandomForestClassifier(random_state=42))])\n",
    "trained_rf, _ = train_model(rf_model, X_train, y_train, cv=cv)\n",
    "metrics = evaluate_model(trained_rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'model__min_samples_split': 8, 'model__n_estimators': 500}\n",
      "-------------------Best model performance --------------------------\n",
      "Score of the model on the train set:\n",
      "1.000 +/- 0.000026\n",
      "Score of the model on the validation set:\n",
      "0.768 +/- 0.003582\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'model__min_samples_split':[4,8,10,12], \n",
    "            'model__n_estimators':[300, 350, 400, 500]}\n",
    "\n",
    "best_model, cv_results = tune_model(rf_model, param_grid, cv=cv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "\n",
    "Eventhough the class imbalance is not severe, it might be worth while to try oversampling with SMOTE\n",
    "\n",
    "Note: - Only the models with hyperparameters which were found to perform well earlier is used here. The reason why HP tuning was not done after Oversampling is due to the difficulty in incorporating SMOTE into model pipelines. If they are not integrated into pipelines, data leakage will occur while doing cross validation and therefore the results will be unrealiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cat_features + num_features\n",
    "\n",
    "smote = SMOTENC(categorical_features=list(range(len(cat_features))))\n",
    "\n",
    "\n",
    "X_train_selected = X_train[features]\n",
    "X_test_selected = X_test[features]\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "X_train_imputed = imputer.fit_transform(X_train_selected)\n",
    "X_test_imputed = imputer.transform(X_test_selected)\n",
    "\n",
    "X_train_os, y_train_os = smote.fit_resample(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36550, 31)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_os.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline for categorical features\n",
    "\n",
    "indices = list(range(len(features)))\n",
    "# select the categorical features from the input\n",
    "select_cat_features = ColumnTransformer([('select_cat', 'passthrough', indices[:len(cat_features)])])\n",
    "\n",
    "#OneHot encoding\n",
    "cat_transformers = Pipeline([('selector', select_cat_features),\n",
    "                            ('onehot', OneHotEncoder(handle_unknown='infrequent_if_exist')),\n",
    "                            ])\n",
    "\n",
    "## Pipeline for numerical featurees\n",
    "\n",
    "# select the numerical variables                            \n",
    "select_num_features = ColumnTransformer([('select_num', 'passthrough', indices[len(cat_features):])])\n",
    "\n",
    "# Imputing for Scaling\n",
    "num_transformers = Pipeline([('selector', select_num_features),\n",
    "                            ('scaler', StandardScaler()),\n",
    "                            ])\n",
    "\n",
    "## combining both pipelines\n",
    "preprocess_pipe = FeatureUnion([('cat', cat_transformers),\n",
    "                                ('num', num_transformers),\n",
    "                                ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC score on test set:  0.7578638780465548\n",
      "Accuracy score on test set:  0.7295007564296521\n"
     ]
    }
   ],
   "source": [
    "rf_model = Pipeline([(\"preprocessing\", preprocess_pipe),\n",
    "                    (\"model\", RandomForestClassifier(n_estimators = 500,\n",
    "                                                    min_samples_split = 8,\n",
    "                                                    random_state=42))])\n",
    "\n",
    "trained_rf = rf_model.fit(X_train_os, y_train_os)\n",
    "metrics = evaluate_model(trained_rf, X_test_imputed, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC score on test set:  0.7637562706902888\n",
      "Accuracy score on test set:  0.7413010590015129\n"
     ]
    }
   ],
   "source": [
    "xgb_model = Pipeline([(\"preprocessing\", preprocess_pipe),\n",
    "                    (\"model\", XGBClassifier(n_estimators = 100,\n",
    "                                            max_depth = 5,\n",
    "                                            random_state=42))])\n",
    "\n",
    "trained_xgb = xgb_model.fit(X_train_os, y_train_os)\n",
    "metrics = evaluate_model(trained_xgb, X_test_imputed, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling hasn't improved our models. Therefore we will stick with our old models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Test-tmAUNUap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d3841023f8ff1c2e6fb1e95f481891f79536ffda216b404d29653bfa4189526"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
